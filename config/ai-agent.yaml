active_pipeline: null
asterisk:
  app_name: asterisk-ai-voice-agent
audio_transport: audiosocket
audiosocket:
  format: slin
  host: 127.0.0.1
  port: 8090
barge_in:
  enabled: true
  initial_protection_ms: 200
  min_ms: 250
  energy_threshold: 1000
  cooldown_ms: 500
  pipeline_min_ms: 120
  pipeline_energy_threshold: 300
  pipeline_talk_detect_enabled: true
  pipeline_talk_detect_silence_ms: 1200
  pipeline_talk_detect_talking_threshold: 128
  post_tts_end_protection_ms: 250
  greeting_protection_ms: 0
  provider_fallback_enabled: true
  provider_fallback_providers:
    - google_live
    - deepgram
  provider_output_suppress_ms: 1200
  provider_output_suppress_extend_ms: 600
  provider_output_suppress_chunk_extend_ms: 250
config_version: 6
farewell_hangup_delay_sec: 3
contexts:
  default:
    greeting: Hi {caller_name}, welcome to Asterisk AI Voice Agent.
    profile: telephony_ulaw_8k
    provider: local_hybrid
    tools:
      - hangup_call
    prompt: >-
      You are Ava, the default assistant for Asterisk AI Voice Agent.
      Congratulate the caller on successfully setting up the project.
      Keep responses concise (1 to 2 sentences), friendly, and conversational.
      If they ask what to do next, guide them to the Admin UI Setup Wizard on port 3003
      to configure providers, contexts, and tools.
      When the caller says they are done, give a short farewell and use the hangup_call tool.
default_provider: local_hybrid
downstream_mode: stream
external_media:
  codec: ulaw
  direction: both
  format: slin16
  port_range: '18080:18099'
  rtp_host: 127.0.0.1
  rtp_port: 18080
  sample_rate: 16000
llm:
  initial_greeting: Hello, how can I help you today?
  prompt: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
pipelines:
  local_hybrid:
    llm: openai_llm
    options:
      llm:
        base_url: https://api.openai.com/v1
        max_tokens: 200
        model: gpt-4o-mini
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tools: null
    tts: local_tts
  local_hybrid_groq:
    llm: groq_llm
    options:
      llm:
        base_url: https://api.groq.com/openai/v1
        max_tokens: 200
        model: llama-3.3-70b-versatile
        temperature: 0.7
      stt:
        chunk_ms: 160
        mode: stt
        stream_format: pcm16_16k
        streaming: true
      tts:
        mode: tts
        response_timeout_sec: 30
        format:
          encoding: mulaw
          sample_rate: 8000
    stt: local_stt
    tts: local_tts
profiles:
  default: telephony_ulaw_8k
  openai_realtime_24k:
    chunk_ms: 20
    idle_cutoff_ms: 0
    internal_rate_hz: 24000
    provider_pref:
      input_encoding: pcm16
      input_sample_rate_hz: 24000
      output_encoding: pcm16
      output_sample_rate_hz: 24000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_responsive:
    chunk_ms: auto
    idle_cutoff_ms: 600
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: slin
      sample_rate_hz: 8000
  telephony_ulaw_8k:
    chunk_ms: auto
    idle_cutoff_ms: 800
    internal_rate_hz: 8000
    provider_pref:
      input_encoding: mulaw
      input_sample_rate_hz: 8000
      output_encoding: mulaw
      output_sample_rate_hz: 8000
    transport_out:
      encoding: ulaw
      sample_rate_hz: 8000
  wideband_pcm_16k:
    chunk_ms: auto
    idle_cutoff_ms: 1200
    internal_rate_hz: 16000
    provider_pref:
      input_encoding: linear16
      input_sample_rate_hz: 16000
      output_encoding: linear16
      output_sample_rate_hz: 16000
    transport_out:
      encoding: slin16
      sample_rate_hz: 16000
providers:
  deepgram:
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    enabled: false
    greeting: Hello, how can I help you today?
    input_encoding: mulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: Voice assistant. Answer in 5-8 words. Be direct. Expand only if asked.
    model: nova-2
    output_encoding: mulaw
    output_sample_rate_hz: 8000
    tts_model: aura-2-thalia-en
    type: full
  google_live:
    api_key: ${GOOGLE_API_KEY}
    name: google_live
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    enable_input_transcription: true
    enable_output_transcription: true
    enabled: false
    greeting: >-
      ${GOOGLE_LIVE_GREETING:-Hi! I'm powered by Google Gemini Live API. Try
      interrupting me!}
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    llm_max_output_tokens: 768
    llm_model: gemini-2.5-flash-native-audio-latest
    llm_temperature: 0.4
    llm_top_k: 20
    llm_top_p: 0.9
    # Google Live: marker-driven hangup detects farewell patterns and hangs up automatically.
    # Preferred over tool-driven hangup which triggers Google 1008 disconnects with native audio models.
    hangup_markers_enabled: true
    # Google Live: disable WebSocket keepalive by default. We saw 1008 disconnects correlated with ping/keepalive
    # behavior; opt-in if needed once verified for your account/model.
    ws_keepalive_enabled: false
    output_encoding: linear16
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 16000
    response_modalities: audio
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    tts_voice_name: Aoede
    type: full
    # Server-side VAD tuning (realtimeInputConfig.automaticActivityDetection)
    # Controls how Google detects speech start/end in the audio stream.
    vad_start_of_speech_sensitivity: START_SENSITIVITY_HIGH
    vad_end_of_speech_sensitivity: END_SENSITIVITY_HIGH
    vad_prefix_padding_ms: 20
    vad_silence_duration_ms: 500
  local:
    # Local AI Server WebSocket URL. Default deployment uses host networking, so 127.0.0.1 is correct.
    # If you run containers on a user-defined bridge network (no host networking), use ws://local_ai_server:8765.
    base_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - stt
      - llm
      - tts
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    continuous_input: true
    enabled: false
    farewell_mode: ${LOCAL_FAREWELL_MODE:=asterisk}
    farewell_timeout_sec: ${LOCAL_FAREWELL_TIMEOUT:=30.0}
    greeting: Hello! I'm your local AI assistant running entirely on-premises.
    instructions: >-
      You are a helpful voice assistant running locally. Be concise and
      friendly.
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 64
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_model: models/stt/vosk-model-en-us-0.22
    temperature: 0.4
    tts_voice: /app/models/tts/en_US-lessac-medium.onnx
    kokoro_model_path: /app/models/tts/kokoro
    tts_backend: piper
    type: full
  local_llm:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - llm
    enabled: false
    llm_model: models/llm/phi-3-mini-4k-instruct.Q4_K_M.gguf
    max_tokens: 32
    temperature: 0.4
    type: local
    # Uses LOCAL_WS_URL (see providers.local.base_url note above).
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_stt:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - stt
    chunk_ms: ${LOCAL_WS_CHUNK_MS:=320}
    connect_timeout_sec: ${LOCAL_WS_CONNECT_TIMEOUT:=2.0}
    enabled: false
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    stt_backend: vosk
    stt_model: models/stt/vosk-model-en-us-0.22
    type: local
    # Uses LOCAL_WS_URL (see providers.local.base_url note above).
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  local_tts:
    auth_token: ${LOCAL_WS_AUTH_TOKEN:-}
    capabilities:
      - tts
    enabled: false
    response_timeout_sec: ${LOCAL_WS_RESPONSE_TIMEOUT:=10.0}
    tts_voice: models/tts/en_US-lessac-medium.onnx
    type: local
    # Uses LOCAL_WS_URL (see providers.local.base_url note above).
    ws_url: ${LOCAL_WS_URL:-ws://127.0.0.1:8765}
  openai_llm:
    capabilities:
      - llm
    api_key: ${OPENAI_API_KEY}
    chat_base_url: https://api.openai.com/v1
    chat_model: gpt-4o-mini
    enabled: false
    response_timeout_sec: 5
    temperature: 0.7
    type: openai
  groq_llm:
    capabilities:
      - llm
    api_key: ${GROQ_API_KEY}
    chat_base_url: https://api.groq.com/openai/v1
    chat_model: llama-3.3-70b-versatile
    enabled: false
    response_timeout_sec: 10
    temperature: 0.7
    tools_enabled: false
    type: openai
  groq_stt:
    capabilities:
      - stt
    type: groq
    enabled: false
    # API key comes from GROQ_API_KEY env var
    stt_base_url: https://api.groq.com/openai/v1/audio/transcriptions
    stt_model: whisper-large-v3-turbo
    response_format: json
    temperature: 0
    request_timeout_sec: 15
  groq_tts:
    capabilities:
      - tts
    type: groq
    enabled: false
    # API key comes from GROQ_API_KEY env var
    tts_base_url: https://api.groq.com/openai/v1/audio/speech
    tts_model: canopylabs/orpheus-v1-english
    voice: hannah
    response_format: wav
    max_input_chars: 200
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    chunk_size_ms: 20
    request_timeout_sec: 15
  ollama_llm:
    capabilities:
      - llm
    base_url: http://localhost:11434
    model: llama3.2
    enabled: false
    temperature: 0.7
    max_tokens: 200
    timeout_sec: 60
    tools_enabled: true
    type: ollama
  openai_realtime:
    base_url: wss://api.openai.com/v1/realtime
    capabilities:
      - stt
      - llm
      - tts
    continuous_input: true
    egress_pacer_enabled: false
    egress_pacer_warmup_ms: 320
    enabled: false
    greeting: Hello, how can I help you today?
    input_encoding: ulaw
    input_gain_max_db: 0
    input_gain_target_rms: 0
    input_sample_rate_hz: 8000
    instructions: You are a voice assistant. Always speak your responses out loud.
    max_response_output_tokens: 4096
    api_version: beta
    model: gpt-4o-realtime-preview-2024-12-17
    organization: ''
    output_encoding: mulaw
    output_sample_rate_hz: 24000
    provider_input_encoding: linear16
    provider_input_sample_rate_hz: 24000
    response_modalities:
      - audio
      - text
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    temperature: 0.6
    turn_detection:
      create_response: true
      prefix_padding_ms: 300
      silence_duration_ms: 1000
      threshold: 0.5
      type: server_vad
    type: openai_realtime
    voice: alloy
  elevenlabs_agent:
    type: full
    enabled: false
    capabilities:
      - stt
      - llm
      - tts
    input_encoding: ulaw
    input_sample_rate_hz: 8000
    provider_input_encoding: pcm16
    provider_input_sample_rate_hz: 16000
    output_encoding: pcm16
    output_sample_rate_hz: 16000
    target_encoding: ulaw
    target_sample_rate_hz: 8000
    voice_id: uDsPstFWFBUXjIBimV7s
    model_id: eleven_flash_v2_5
    voice_settings:
      stability: 0.5
      similarity_boost: 0.75
      style: 0
      use_speaker_boost: true
    greeting: Hello! I'm your ElevenLabs voice assistant. How can I help you today?
    instructions: You are a helpful voice assistant. Be concise and friendly.
    continuous_input: true
    input_gain_target_rms: 0
    input_gain_max_db: 0
  openai_stt:
    enabled: false
    capabilities:
      - stt
    input_encoding: linear16
    input_sample_rate_hz: 16000
    stt_base_url: https://api.openai.com/v1/audio/transcriptions
    stt_model: whisper-1
    response_format: json
    temperature: 0
    request_timeout_sec: 15
    type: openai
  openai_tts:
    enabled: false
    capabilities:
      - tts
    response_format: wav
    target_encoding: mulaw
    target_sample_rate_hz: 8000
    tts_base_url: https://api.openai.com/v1/audio/speech
    tts_model: tts-1
    type: openai
    voice: alloy
streaming:
  chunk_size_ms: 20
  connection_timeout_ms: 120000
  continuous_stream: true
  diag_enable_taps: true
  diag_out_dir: /tmp/ai-engine-taps
  diag_post_secs: 1
  diag_pre_secs: 1
  empty_backoff_ticks_max: 5
  fallback_timeout_ms: 8000
  greeting_min_start_ms: 40
  # ExternalMedia greeting reliability: wait briefly for inbound RTP to establish the remote endpoint,
  # then fall back to file playback if RTP is still not routable (Asterisk may not emit RTP until speech).
  greeting_rtp_wait_ms: 250
  jitter_buffer_ms: 950
  keepalive_interval_ms: 5000
  low_watermark_ms: 80
  min_start_ms: 120
  normalizer:
    enabled: true
    max_gain_db: 18
    target_rms: 1400
  provider_grace_ms: 200
  sample_rate: 8000

# ============================================================================
# In-Call HTTP Tools (AI-invokable during conversation)
# ============================================================================
in_call_tools:
  # Example: n8n intent router (in-call) - AI sends the user message to an n8n webhook
  # Disabled by default. Enable and set URL to test.
  sample_n8n_in_call_tool:
    kind: in_call_http_lookup
    enabled: false
    is_global: false
    description: "Example in-call tool: send user message to an n8n webhook and return a reply."
    timeout_ms: 5000
    url: "https://your-n8n-instance.com/webhook/intent-router"
    method: POST
    headers:
      Content-Type: "application/json"
      # Authorization: "Bearer ${N8N_API_KEY}"
    body_template: |
      {
        "call_id": "{call_id}",
        "caller_number": "{caller_number}",
        "context": "{context_name}",
        "userMessage": "{userMessage}"
      }
    parameters:
      - name: userMessage
        type: string
        description: "The user message to send to the webhook."
        required: true
    output_variables:
      reply: "reply"
      intent: "intent"
    return_raw_json: false
    error_message: "I couldn't reach the automation service right now. Please try again."
tools:
  ai_identity:
    name: AI Agent
    number: '6789'
  
  # ============================================================================
  # Phase Tools (Milestone 24) - Pre-call and Post-call HTTP integrations
  # ============================================================================

  # Example: GoHighLevel pre-call lookup (context-specific)
  # Disabled by default. Enable and set URL/headers for your GHL account to test.
  sample_gohighlevel_pre_call_lookup:
    kind: generic_http_lookup
    phase: pre_call
    enabled: false
    is_global: false
    timeout_ms: 5000
    hold_audio_file: "custom/please-wait"
    hold_audio_threshold_ms: 500
    url: "https://services.leadconnectorhq.com/contacts/search"
    method: POST
    headers:
      Content-Type: "application/json"
      # Authorization: "Bearer ${GHL_API_KEY}"
      # Version: "2021-07-28"
    body_template: |
      {
        "query": "{caller_number}"
      }
    output_variables:
      ghl_contact_id: "contacts[0].id"
      ghl_contact_name: "contacts[0].name"
      ghl_contact_email: "contacts[0].email"
  
  # Example: Post-call webhook (global - fires for all calls)
  # Sends call data to external system after call ends
  demo_post_call_webhook:
    kind: generic_webhook
    phase: post_call
    enabled: false  # Set to true and configure URL to test
    is_global: true
    timeout_ms: 5000
    url: "https://your-webhook-endpoint.com/call-completed"
    method: POST
    headers:
      Content-Type: "application/json"
      # Authorization: "Bearer ${WEBHOOK_API_KEY}"
    payload_template: |
      {
        "schema_version": 1,
        "event_type": "call_completed",
        "call_id": "{call_id}",
        "caller_number": "{caller_number}",
        "caller_name": "{caller_name}",
        "call_duration": {call_duration},
        "call_outcome": "{call_outcome}",
        "transcript": {transcript_json},
        "context": "{context_name}",
        "provider": "{provider}",
        "timestamp": "{call_end_time}"
      }

  # Example: Discord webhook - posts call summary to a Discord channel (context-specific)
  # Disabled by default. Enable and add your Discord webhook URL to test.
  sample_discord_post_call_webhook:
    kind: generic_webhook
    phase: post_call
    enabled: false
    is_global: false
    timeout_ms: 5000
    url: "https://discord.com/api/webhooks/YOUR_WEBHOOK_ID/YOUR_WEBHOOK_TOKEN"
    method: POST
    headers:
      Content-Type: "application/json"
    payload_template: |
      {
        "content": "Call completed\\nContext: {context_name}\\nCaller: {caller_number} ({caller_name})\\nDuration: {call_duration}s\\nOutcome: {call_outcome}\\nProvider: {provider}\\n\\nSummary: {summary}"
      }
    generate_summary: true

  # Example: Discord webhook - posts call summary to a Discord channel
  # discord_webhook:
  #   kind: generic_webhook
  #   phase: post_call
  #   enabled: false  # Set to true and add your webhook URL
  #   is_global: true
  #   timeout_ms: 5000
  #   url: "https://discord.com/api/webhooks/YOUR_WEBHOOK_ID/YOUR_WEBHOOK_TOKEN"
  #   method: POST
  #   headers:
  #     Content-Type: "application/json"
  #   payload_template: |
  #     {"content": "ðŸ“ž **Call Completed**\n\n**Duration:** {call_duration}s\n**Outcome:** {call_outcome}\n**Context:** {context_name}\n**Provider:** {provider}\n**Time:** {call_end_time}\n\n**Summary:**\n{summary}"}
  #   generate_summary: true

  # Example: n8n workflow webhook - triggers n8n automation after calls
  # n8n_webhook:
  #   kind: generic_webhook
  #   phase: post_call
  #   enabled: false  # Set to true and add your n8n webhook URL
  #   is_global: true
  #   timeout_ms: 5000
  #   url: "https://your-n8n-instance.com/webhook/your-webhook-path"
  #   method: POST
  #   headers:
  #     Content-Type: "application/json"
  #   payload_template: |
  #     {
  #       "event_type": "call_completed",
  #       "call_id": "{call_id}",
  #       "caller_number": "{caller_number}",
  #       "caller_name": "{caller_name}",
  #       "call_duration": {call_duration},
  #       "call_outcome": "{call_outcome}",
  #       "summary": "{summary}",
  #       "context": "{context_name}",
  #       "provider": "{provider}",
  #       "timestamp": "{call_end_time}"
  #     }
  #   generate_summary: true

  # Example: Pre-call CRM lookup (context-specific)
  # Fetches customer data before AI speaks
  # demo_crm_lookup:
  #   kind: generic_http_lookup
  #   phase: pre_call
  #   enabled: false
  #   is_global: false
  #   timeout_ms: 2000
  #   hold_audio_file: "custom/please-wait"  # Asterisk sound file
  #   hold_audio_threshold_ms: 500
  #   url: "https://api.example.com/contacts/lookup"
  #   method: GET
  #   headers:
  #     Authorization: "Bearer ${CRM_API_KEY}"
  #   query_params:
  #     phone: "{caller_number}"
  #   output_variables:
  #     customer_name: "contact.name"
  #     customer_email: "contact.email"
  #     account_status: "contact.status"
  cancel_transfer:
    allow_after_answer: false
    allow_during_ring: true
    enabled: false
  default_action_timeout: 30
  enabled: true
  extensions:
    internal:
      '6000':
        action_type: transfer
        device_state_tech: auto
        aliases:
          - agent
          - representative
          - human
          - real person
          - live person
          - someone
          - support
          - sales
          - operator
          - help desk
        description: Live customer service representative
        dial_string: SIP/6000
        mode: warm
        name: Live Agent
        pass_caller_info: true
        timeout: 30
        transfer: true
  hangup_call:
    enabled: true
    farewell_message: Thank you for calling. Goodbye!
    require_confirmation: false
    policy:
      mode: normal
      enforce_transcript_offer: false
      block_during_contact_capture: true
      markers:
        end_call:
          - no transcript
          - no transcript needed
          - don't send a transcript
          - do not send a transcript
          - no need for a transcript
          - no thanks
          - no thank you
          - that's all
          - that is all
          - that's it
          - that is it
          - nothing else
          - all set
          - all good
          - end the call
          - end call
          - hang up
          - hangup
          - goodbye
          - bye
        assistant_farewell:
          - goodbye
          - bye
          - thank you for calling
          - thanks for calling
          - have a great day
          - have a good day
          - take care
          - ending the call
          - i'll let you go
        affirmative:
          - yes
          - yeah
          - yep
          - correct
          - that's correct
          - thats correct
          - that's right
          - thats right
          - right
          - exactly
          - affirmative
        negative:
          - no
          - nope
          - nah
          - negative
          - don't
          - dont
          - do not
          - not
          - not needed
          - no need
          - no thanks
          - no thank you
          - decline
          - skip
  leave_voicemail:
    enabled: false
    extension: '2765'
  request_transcript:
    admin_email: ''
    api_key: ${REQUEST_TRANSCRIPT_API_KEY:-}
    common_domains:
      - gmail.com
      - yahoo.com
      - outlook.com
      - hotmail.com
      - icloud.com
    confirm_email: true
    enabled: false
    from_email: ${REQUEST_TRANSCRIPT_FROM_EMAIL:-noreply@example.com}
    from_name: ${REQUEST_TRANSCRIPT_FROM_NAME:-Asterisk AI Voice Agent}
    max_attempts: 2
    provider: ${REQUEST_TRANSCRIPT_PROVIDER:-resend}
    validate_domain: false
  send_email_summary:
    admin_email: ${SEND_EMAIL_ADMIN:-}
    api_key: ${SEND_EMAIL_API_KEY:-}
    enabled: false
    from_email: ${SEND_EMAIL_FROM:-noreply@example.com}
    from_name: ${SEND_EMAIL_FROM_NAME:-Asterisk AI Voice Agent}
    include_metadata: true
    include_transcript: true
    provider: ${SEND_EMAIL_PROVIDER:-resend}
  transfer:
    technology: PJSIP
    destinations:
      sales_agent:
        description: Sales agent
        target: '2765'
        type: extension
        live_agent: false
      sales_queue: null
      sales_team: null
      support_agent: null
      support_queue: null
      support_team: null
    enabled: false
mcp:
  enabled: false
  servers:
    weather:
      transport: stdio
      command:
        - python3
        - '-m'
        - src.mcp_servers.weather_mcp_server
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me check the weather for you, one moment...
      tools:
        - name: get_weather_by_city
          expose_as: mcp_weather_get_city
          speech_field: spoken
    aviation_atis:
      transport: stdio
      command:
        - python3
        - '-m'
        - src.mcp_servers.aviation_atis_server
        - '--config'
        - /app/config/aviation_atis.yaml
      env:
        METNO_USER_AGENT: >-
          Asterisk-AI-Voice-Agent
          (+https://github.com/hkjarral/Asterisk-AI-Voice-Agent)
      defaults:
        timeout_ms: 15000
        slow_response_threshold_ms: 3000
        slow_response_message: Let me get the current ATIS for you, one moment...
      tools:
        - name: get_atis
          expose_as: mcp_aviation_atis
          description: Get current ATIS for an ICAO airport code (e.g., LSMP, KJFK)
          speech_field: atis_text
vad:
  enhanced_enabled: true
  fallback_buffer_size: 128000
  fallback_enabled: true
  fallback_interval_ms: 4000
  max_utterance_duration_ms: 10000
  min_utterance_duration_ms: 600
  use_provider_vad: false
  utterance_padding_ms: 200
  webrtc_aggressiveness: 1
  webrtc_end_silence_frames: 50
  webrtc_start_frames: 3
